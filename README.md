# Autoencoder_data_denoising
Dirty Data denoising with autoencoder


## Objective: 
This projects cleans noisy images of text data using an AutoEncoder with Keras and TensorFlow Backend.

## Dataset 
The dataset can be found in Kaggle website: https://www.kaggle.com/c/denoising-dirty-documents


## Introduction to denoising of dirty textdata

Numerous scientific papers, historical documentaries/artifacts, recipes, books are stored as papers be it handwritten/typewritten. With time, the paper/notes tend to accumulate noise/dirt through fingerprints, weakening of paper fibers, dirt, coffee/tea stains, abrasions, wrinkling, etc. There are several surface cleaning methods used for both preserving and cleaning, but they have certain limits, the major one being: that the original document might get altered during the process.
I along with Michael Lally and Kartikeya Shukla worked on the data set of noisy documents if from the UC Irvine NoisyOffice Data Set. Denoising dirty documents enables the creation of higher fidelity digital recreations of original documents. Several methods for denoising documents like â€” Median Filtering, Edge Detection, Dilation & Erosion, Adaptive Filtering, Autoencoding, and Linear Regression are applied to a test dataset and their results are evaluated, discussed, and compared.

